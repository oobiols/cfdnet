@article{chambers2004simulated,
  title={Simulated apoptosis/neurogenesis regulates learning and memory capabilities of adaptive neural networks},
  author={Chambers, R Andrew and Potenza, Marc N and Hoffman, Ralph E and Miranker, Willard},
  journal={Neuropsychopharmacology},
  volume={29},
  number={4},
  pages={747--758},
  year={2004}
}

@article{ghiassi2005dynamic,
  title={A dynamic artificial neural network model for forecasting time series events},
  author={Ghiassi, M and Saidane, H and Zimbra, DK},
  journal={International Journal of Forecasting},
  volume={21},
  number={2},
  pages={341--362},
  year={2005},
  publisher={Elsevier}
}

@article{leuner2006there,
  title={Is there a link between adult neurogenesis and learning?},
  author={Leuner, Benedetta and Gould, Elizabeth and Shors, Tracey J},
  journal={Hippocampus},
  volume={16},
  number={3},
  pages={216--224},
  year={2006},
  publisher={Wiley Online Library}
}

@article{aimone2009computational,
  title={Computational influence of adult neurogenesis on memory encoding},
  author={Aimone, James B and Wiles, Janet and Gage, Fred H},
  journal={Neuron},
  volume={61},
  number={2},
  pages={187--202},
  year={2009},
  publisher={Elsevier}
}

@TECHREPORT{citeulike:13837097,
    author = {Steve Lawrence and C. Lee Giles and Ah Chung Tsoi},
    title = {What size neural network gives optimal generalization? convergence properties of backpropagation},
    institution = {NEC Research Institute and University of Queensland},
    year = {1996}
}

@article{murray2015auto,
  title={Auto-Sizing Neural Networks: With Applications to n-gram Language Models},
  author={Murray, Kenton and Chiang, David},
  journal={arXiv preprint arXiv:1508.05051},
  year={2015}
}

@Inbook{Kowaliw2014,
  author="Kowaliw, Taras and Bredeche, Nicolas and Chevallier, Sylvain and Doursat, Ren{\'e}",
  editor="Kowaliw, Taras and Bredeche, Nicolas and Doursat, Ren{\'e}",
  title="Artificial Neurogenesis: An Introduction and Selective Review",
  bookTitle="Growing Adaptive Machines: Combining Development and Learning in Artificial Neural Networks",
  year="2014",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="1--60",
  isbn="978-3-642-55337-0",
  doi="10.1007/978-3-642-55337-0_1",
  url="http://dx.doi.org/10.1007/978-3-642-55337-0_1"
}

@article{aimone200822,
  title={Computational Modeling of Adult Neurogenesis},
  author={Aimone, James B and Wiskott, Laurenz},
  journal={Cold Spring Harbor Monograph Archive},
  volume={52},
  pages={463--481},
  year={2008}
}

@article {HIPO:HIPO20167,
author = {Wiskott, Laurenz and Rasch, Malte J. and Kempermann, Gerd},
title = {A functional hypothesis for adult hippocampal neurogenesis: Avoidance of catastrophic interference in the dentate gyrus},
journal = {Hippocampus},
volume = {16},
number = {3},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
issn = {1098-1063},
url = {http://dx.doi.org/10.1002/hipo.20167},
doi = {10.1002/hipo.20167},
pages = {329--343},
keywords = {hippocampus, network model, autoencoder network, adaptation},
year = {2006},
}

@inproceedings{fahlman1989cascade,
  title={The Cascade-Correlation Learning Architecture},
  author={Scott Fahlman and Lebiere, Christian},
  booktitle={Advances in Neural Information Processing Systems 2},
  year={1990},
  organization={Citeseer}
}

@article{carpenter1987massively,
  title={A massively parallel architecture for a self-organizing neural pattern recognition machine},
  author={Carpenter, Gail A and Grossberg, Stephen},
  journal={Computer vision, graphics, and image processing},
  volume={37},
  number={1},
  pages={54--115},
  year={1987},
  publisher={Elsevier}
}

@ARTICLE{2016arXiv160701097C,
   author = {{Cortes}, C. and {Gonzalvo}, X. and {Kuznetsov}, V. and {Mohri}, M. and 
	{Yang}, S.},
    title = "{AdaNet: Adaptive Structural Learning of Artificial Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1607.01097},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2016,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160701097C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{stanley2002evolving,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  publisher={MIT Press}
}

@article{siddiquee2010constructive,
  title={A Constructive Algorithm for Feedforward Neural Networks for Medical Diagnostic Reasoning},
  author={Siddiquee, Abu Bakar and Mazumder, Md and Hoque, Ehsanul and Kamruzzaman, SM},
  journal={arXiv preprint arXiv:1009.4564},
  year={2010}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1135--1143},
  year={2015}
}

@incollection{carpenter2002adaptative,
	editor = {M. Arbib},
	pages = {87},
	year = {2002},
	author = {Gail Carpenter and Stephen Grossberg},
	booktitle = {The Handbook of Brain Theory and Neural Networks},
	publisher = {MIT Press},
	title = {Adaptative Resonance Theory}
}

@article{grossberg1976adaptive,
  title={Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors},
  author={Grossberg, Stephen},
  journal={Biological cybernetics},
  volume={23},
  number={3},
  pages={121--134},
  year={1976},
  publisher={Springer}
}

@article{lin2016constructive,
  title={Constructive neural network learning},
  author={Lin, Shaobo and Zeng, Jinshan and Zhang, Xiaoqin},
  journal={arXiv preprint arXiv:1605.00079},
  year={2016}
}

@article{kwok1997constructive,
  title={Constructive algorithms for structure learning in feedforward neural networks for regression problems},
  author={Kwok, Tin-Yau and Yeung, Dit-Yan},
  journal={IEEE Transactions on Neural Networks},
  volume={8},
  number={3},
  pages={630--645},
  year={1997},
  publisher={IEEE}
}

@inproceedings{43022,
title = {Going Deeper with Convolutions},
author  = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year  = 2015,
URL = {http://arxiv.org/abs/1409.4842},
booktitle = {CVPR 2015}
}

@incollection{NIPS2012_4824,
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 author = {Alex Krizhevsky and Sutskever, Ilya and Geoffrey E. Hinton},
 booktitle = {Advances in Neural Information Processing Systems 25},
 editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
 pages = {1097--1105},
 year = {2012},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{lin2013network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1312.4400},
  year={2013}
}

@INPROCEEDINGS{7840668,
	author={C. Siegel and J. Daily and A. Vishnu},
	booktitle={2016 IEEE International Conference on Big Data (Big Data)},
	title={Adaptive neuron apoptosis for accelerating deep learning on large scale systems},
	year={2016},
	pages={753-762},
	keywords={Big Data;large-scale systems;learning (artificial intelligence);neural nets;Higgs Boson dataset;ImageNet classification;adaptive neuron apoptosis;deep learning acceleration;large scale systems;low overhead removal;redundant neurons;Biological neural networks;Machine learning;Network topology;Neurons;Topology;Training},
	doi={10.1109/BigData.2016.7840668},
	month={Dec},
}

@inproceedings{VNHKH,
  author    = {Abhinav Vishnu and
               Jeyanthi Narasimhan and
               Lawrence Holder and
               Darren J. Kerbyson and
               Adolfy Hoisie},
  title     = {Fast and Accurate Support Vector Machines on Large Scale Systems},
  booktitle = {2015 {IEEE} International Conference on Cluster Computing, {CLUSTER}
               2015, Chicago, IL, USA, September 8-11, 2015},
  pages     = {110--119},
  year      = {2015},
  IGNOREcrossref  = {DBLP:conf/cluster/2015},
  url       = {http://dx.doi.org/10.1109/CLUSTER.2015.26},
  doi       = {10.1109/CLUSTER.2015.26},
  timestamp = {Thu, 05 Nov 2015 13:26:07 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cluster/VishnuNHKH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Duchi,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {2121--2159},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
 acmid = {2021068},
 publisher = {JMLR.org},
} 

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@article{adultref,
 author = {Deepajothi, S and Selvarajan, S},
 title = {A Comparative Study of Classification Techniques on Adult Data Set},
 journal = {International Journal of Engineering Research and Yechnology},
 issue_date = {10/2012},
 volume = {1},
 month = oct,
 year = {2012},
 url = {http://www.ijert.org/view-pdf/1422/a-comparative-study-of-classification-techniques-on-adult-data-set},
 issn = {2278-0181},
} 

@article{Bengio:2009:LDA:1658423.1658424,
 author = {Bengio, Yoshua},
 title = {Learning Deep Architectures for AI},
 journal = {Found. Trends Mach. Learn.},
 issue_date = {January 2009},
 volume = {2},
 number = {1},
 month = jan,
 year = {2009},
 issn = {1935-8237},
 pages = {1--127},
 numpages = {127},
 url = {http://dx.doi.org/10.1561/2200000006},
 doi = {10.1561/2200000006},
 acmid = {1658424},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@article{Hinton:2002:TPE:639729.639730,
 author = {Hinton, Geoffrey E.},
 title = {Training Products of Experts by Minimizing Contrastive Divergence},
 journal = {Neural Comput.},
 issue_date = {August 2002},
 volume = {14},
 number = {8},
 month = aug,
 year = {2002},
 issn = {0899-7667},
 pages = {1771--1800},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976602760128018},
 doi = {10.1162/089976602760128018},
 acmid = {639730},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{Lecun98gradient-basedlearning,
    author = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{Suykens:1999:LSS:326394.326408,
 author = {Suykens, J. A. K. and Vandewalle, J.},
 title = {Least Squares Support Vector Machine Classifiers},
 journal = {Neural Process. Lett.},
 issue_date = {June 1999},
 volume = {9},
 number = {3},
 month = jun,
 year = {1999},
 issn = {1370-4621},
 pages = {293--300},
 numpages = {8},
 url = {http://dx.doi.org/10.1023/A:1018628609742},
 doi = {10.1023/A:1018628609742},
 acmid = {326408},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {classification, linear least squares, radial basis function kernel, support vector machines},
} 

@inproceedings{Vedaldi:2015:MCN:2733373.2807412,
 author = {Vedaldi, Andrea and Lenc, Karel},
 title = {MatConvNet: Convolutional Neural Networks for MATLAB},
 booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
 series = {MM '15},
 year = {2015},
 isbn = {978-1-4503-3459-4},
 location = {Brisbane, Australia},
 pages = {689--692},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2733373.2807412},
 doi = {10.1145/2733373.2807412},
 acmid = {2807412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, convolutional neural networks, deep learning, image understanding, machine learning},
} 

@article{Vincent:2010:SDA:1756006.1953039,
 author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
 title = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = dec,
 year = {2010},
 issn = {1532-4435},
 pages = {3371--3408},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1953039},
 acmid = {1953039},
 publisher = {JMLR.org},
} 

@InProceedings{SalHinton07,
  author=         "Ruslan Salakhutdinov and Geoffrey Hinton",
  title=          "Deep {B}oltzmann Machines",
  booktitle=      "Proceedings of the International Conference on Artificial Intelligence and Statistics",
  volume=         "5", 
  pages=		"448-455",
  year=           "2009"
}

@MASTERSTHESIS\{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
    school       = "Technical University of Denmark, {DTU} Informatics, {E-}mail: reception@imm.dtu.dk",
    address      = "Asmussens Alle, Building 305, {DK-}2800 Kgs. Lyngby, Denmark",
    type         = "",
    note         = "Supervised by Associate Professor Ole Winther, owi@imm.dtu.dk, {DTU} Informatics, and Morten M{\o}rup, mm@imm.dtu.dk, {DTU} Informatics",
    url          = "http://www.imm.dtu.dk/English.aspx",
    abstract     = "Recent findings [HOT06] have made possible the learning of deep layered hierarchical representations of data mimicking the brains working. It is hoped that this paradigm will unlock some of the power of the brain and lead to advances towards true {AI}.
In this thesis I implement and evaluate state-of-the-art deep learning models and using these as building blocks I investigate the hypothesis that predicting the time-to-time sensory input is a good learning objective. I introduce the Predictive Encoder (PE) and show that a simple non-regularized learning rule, minimizing prediction error on natural video patches leads to receptive fields similar to those found in Macaque monkey visual area V1. I scale this model to video of natural scenes by introducing the Convolutional Predictive Encoder (CPE) and show similar results. Both models can be used in deep architectures as a deep learning module."
}
 
@techreport{manyika11frontier,
  added-at = {2013-03-15T09:48:25.000+0100},
  author = {Manyika, James and Chui, Michael and Brown, Brad and Bughin, Jacques and Dobbs, Richard and Roxburgh, Charles and Byers, Angela Hung},
  biburl = {http://www.bibsonomy.org/bibtex/2cb919811c0adab2e1d359ce1bdbac9d4/sb3000},
  institution = {McKinsey Global Institute},
  interhash = {117de3444c856242c38ab48d24604b52},
  intrahash = {cb919811c0adab2e1d359ce1bdbac9d4},
  keywords = {analytics bigdata mckinsey},
  month = {June},
  timestamp = {2013-07-01T22:02:25.000+0200},
  title = {Big Data: The Next Frontier for Innovation, Competition, and Productivity},
  year = 2011
}

@article{HintonSalakhutdinov2006b,
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  added-at = {2008-07-15T10:05:18.000+0200},
  author = {Hinton, G E and Salakhutdinov, R R},
  biburl = {http://www.bibsonomy.org/bibtex/2135bbce97b449ddf5fca7be88102b53c/tmalsburg},
  description = {Reducing the dimensionality of data with neural ne...[Science. 2006] - PubMed Result},
  doi = {10.1126/science.1127647},
  interhash = {019918b82518b74f443a22dc58a0117f},
  intrahash = {135bbce97b449ddf5fca7be88102b53c},
  journal = {Science},
  keywords = {dimensionalityreduction neuralnetworks parameterestimation},
  month = Jul,
  number = 5786,
  pages = {504-507},
  pmid = {16873662},
  timestamp = {2008-07-15T10:05:18.000+0200},
  title = {Reducing the dimensionality of data with neural networks},
  url = {http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed&uid=16873662&cmd=showdetailview&indexed=google},
  volume = 313,
  year = 2006
}

@article{gardner1985exponential,
  title={Exponential smoothing: The state of the art},
  author={Gardner, Everette S},
  journal={Journal of forecasting},
  volume={4},
  number={1},
  pages={1--28},
  year={1985},
  publisher={Wiley Online Library}
}

@article{deng2012three,
  title={Three classes of deep learning architectures and their applications: a tutorial survey},
  author={Deng, Li},
  journal={APSIPA transactions on signal and information processing},
  year={2012}
}

@article{Baldi:2014kfa,
      author         = "Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel",
      title          = "{Searching for Exotic Particles in High-Energy Physics
                        with Deep Learning}",
      journal        = "Nature Commun.",
      volume         = "5",
      year           = "2014",
      pages          = "4308",
      doi            = "10.1038/ncomms5308",
      eprint         = "1402.4735",
      archivePrefix  = "arXiv",
      primaryClass   = "hep-ph",
      SLACcitation   = "%%CITATION = ARXIV:1402.4735;%%"
}

@article{DBLP:journals/corr/abs-1009-4983,
  author    = {S. M. Kamruzzaman and
               Ahmed Ryadh Hasan},
  title     = {Pattern Classification using Simplified Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1009.4983},
  year      = {2010},
  url       = {http://arxiv.org/abs/1009.4983},
  timestamp = {Mon, 05 Dec 2011 18:04:05 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1009-4983},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Wang:1993:GCC:155870.155881,
 author = {Wang, Xingwei and Zhao, Hong and Zhu, Jiakeng},
 title = {GRPC: A Communication Cooperation Mechanism in Distributed Systems},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {July 1993},
 volume = {27},
 number = {3},
 month = jul,
 year = {1993},
 issn = {0163-5980},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155870.155881},
 doi = {10.1145/155870.155881},
 acmid = {155881},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@incollection{NIPS2006_3048,
title = {Greedy Layer-Wise Training of Deep Networks},
author = {Bengio, Yoshua and Pascal Lamblin and Dan Popovici and Larochelle, Hugo},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {153--160},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf}
}

@ARTICLE{Hinton06afast,
    author = {Geoffrey E. Hinton and Simon Osindero},
    title = {A fast learning algorithm for deep belief nets},
    journal = {Neural Computation},
    year = {2006},
    volume = {18},
    pages = {2006}
}

@article{Bianchini2014,
	abstract = {Recently, researchers in the artificial neural network field have focused their attention on connectionist models composed by several hidden layers. In fact, experimental results and heuristic considerations suggest that deep architectures are more suitable than shallow ones for modern applications, facing very complex problems, e.g., vision and human language understanding. However, the actual theoretical results supporting such a claim are still few and incomplete. In this paper, we propose a new approach to study how the depth of feedforward neural networks impacts on their ability in implementing high complexity functions. First, a new measure based on topological concepts is introduced, aimed at evaluating the complexity of the function implemented by a neural network, used for classification purposes. Then, deep and shallow neural architectures with common sigmoidal activation functions are compared, by deriving upper and lower bounds on their complexity, and studying how the complexity depends on the number of hidden units and the used activation function. The obtained results seem to support the idea that deep networks actually implements functions of higher complexity, so that they are able, with the same number of resources, to address more difficult problems.},
	affiliation = {, University of Siena, Siena, Italy},
	author = {Bianchini, Monica and Scarselli, Franco},
	doi = {10.1109/TNNLS.2013.2293637},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Complexity theory; Neurons; Biological neural networks; Polynomials; Upper bound; Computer architecture; Vapnik--Chervonenkis dimension (VC-dim).; Betti numbers; deep neural networks; function approximation; topological complexity},
	language = {Undetermined},
	number = {8},
	pages = {1553 - 1565},
	title = {On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures},
	volume = {25},
	year = {2014},
}

@Article{ref1,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="http://dx.doi.org/10.1007/BF02551274",
year="1989"
}

@inproceedings{lee2009convolutional,
  title={Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
  author={Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={609--616},
  year={2009},
  organization={ACM}
}

@article{jia2014caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}

@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
      abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.}
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}

@TechReport {export:226641,
abstract     = {<p>We introduce computational network (CN), a unified framework for describing
                arbitrary learning machines, such as deep neural networks (DNNs), convolutional
                neural networks (CNNs), recurrent neural networks (RNNs), long short term memory
                (LSTM), logistic regression, and maximum entropy model, that can be illustrated
                as a series of computational steps. A CN is a directed graph in which each leaf
                node represents an input value or a parameter and each non-leaf node represents a
                matrix operation upon its children. We describe algorithms to carry out forward
                computation and gradient calculation in CN and introduce most popular computation
                node types used in a typical CN.
We further introduce the computational network
                toolkit (CNTK), an implementation of CN that supports both GPU and CPU. We
                describe the architecture and the key components of the CNTK, the command line
                options to use CNTK, and the network definition and model editing language, and
                provide sample setups for acoustic model, language model, and spoken language
                understanding. We also describe the Argon speech recognition decoder as an
                example to integrate with CNTK.</p>},
author       = {Amit Agarwal and Eldar Akchurin and Chris Basoglu and Guoguo Chen and Scott
                Cyphers and Jasha Droppo and Adam Eversole and Brian Guenter and Mark Hillebrand
                and Ryan Hoens and Xuedong Huang and Zhiheng Huang and Vladimir Ivanov and Alexey
                Kamenev and Philipp Kranen and Oleksii Kuchaiev and Wolfgang Manousek and Avner
                May and Bhaskar Mitra and Olivier Nano and Gaizka Navarro and Alexey Orlov and
                Marko Padmilac and Hari Parthasarathi and Baolin Peng and Alexey Reznichenko and
                Frank Seide and Michael L. Seltzer and Malcolm Slaney and Andreas Stolcke and
                Yongqiang Wang and Huaming Wang and Kaisheng Yao and Dong Yu and Yu Zhang and
                Geoffrey Zweig},
month        = {August},
number       = {MSR-TR-2014-112},
publisher    = {Microsoft Research},
title        = {An Introduction to Computational Networks and the Computational Network Toolkit},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=226641},
year         = {2014},
}

@MISC{Collobert02torch:a,
    author = {Ronan Collobert and Samy Bengio and Johnny Marithoz},
    title = {Torch: A Modular Machine Learning Software Library},
    year = {2002}
}

@article{iandola2015firecaffe,
  title={FireCaffe: near-linear acceleration of deep neural network training on compute clusters},
  author={Iandola, Forrest N and Ashraf, Khalid and Moskewicz, Mattthew W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1511.00175},
  year={2015}
}

@article{he2015deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={arXiv preprint arXiv:1512.03385},
  year={2015}
}

@article{draelos2016neurogenesis,
  title={Neurogenesis Deep Learning},
  author={Draelos, Timothy J and Miner, Nadine E and Lamb, Christopher C and Vineyard, Craig M and Carlson, Kristofor D and James, Conrad D and Aimone, James B},
  journal={arXiv preprint arXiv:1612.03770},
  year={2016}
}

@ARTICLE{matex,
   author = {{Vishnu}, A. and {Siegel}, C. and {Daily}, J.},
    title = "{Distributed TensorFlow with MPI}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1603.02339},
 primaryClass = "cs.DC",
 keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
     year = 2016,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160302339V},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{bergstra2013hyperopt,
  title={Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms},
  author={Bergstra, James and Yamins, Dan and Cox, David D},
  booktitle={Proceedings of the 12th Python in Science Conference},
  pages={13--20},
  year={2013},
  organization={Citeseer}
}

@inproceedings{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2546--2554},
  year={2011}
}

@article{bergstra2013making,
  title={Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures.},
  author={Bergstra, James and Yamins, Daniel and Cox, David D},
  journal={ICML (1)},
  volume={28},
  pages={115--123},
  year={2013}
}

@article{perotti2006scale,
  title={A scale-free neural network for modelling neurogenesis},
  author={Perotti, Juan I and Tamarit, Francisco A and Cannas, Sergio A},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={371},
  number={1},
  pages={71--75},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{lee1990training,
  title={Training of a Neural Network with Topology Generation for the Classification Problem},
  author={Lee, Hahn-Ming and Hsu, Ching-Chi},
  booktitle={International Neural Network Conference},
  pages={787--787},
  year={1990},
  organization={Springer}
}

@article{han2016dsd,
  title={Dsd: Regularizing deep neural networks with dense-sparse-dense training flow},
  author={Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Tang, Shijian and Elsen, Erich and Catanzaro, Bryan and Tran, John and Dally, William J},
  journal={arXiv preprint arXiv:1607.04381},
  year={2016}
}

@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@misc{1704.08792,
Author = {Renato Negrinho and Geoff Gordon},
Title = {DeepArchitect: Automatically Designing and Training Deep Architectures},
Year = {2017},
Eprint = {arXiv:1704.08792},
}

@book{marsland2015machine,
  title={Machine learning: an algorithmic perspective},
  author={Marsland, Stephen},
  year={2015},
  publisher={CRC press}
}

@article{goh2017deep,
  title={Deep learning for computational chemistry},
  author={Goh, Garrett B and Hodas, Nathan O and Vishnu, Abhinav},
  journal={Journal of Computational Chemistry},
  year={2017}
}