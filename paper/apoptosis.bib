@article{chambers2004simulated,
  title={Simulated apoptosis/neurogenesis regulates learning and memory capabilities of adaptive neural networks},
  author={Chambers, R Andrew and Potenza, Marc N and Hoffman, Ralph E and Miranker, Willard},
  journal={Neuropsychopharmacology},
  volume={29},
  number={4},
  pages={747--758},
  year={2004}
}

@article{ghiassi2005dynamic,
  title={A dynamic artificial neural network model for forecasting time series events},
  author={Ghiassi, M and Saidane, H and Zimbra, DK},
  journal={International Journal of Forecasting},
  volume={21},
  number={2},
  pages={341--362},
  year={2005},
  publisher={Elsevier}
}

@article{leuner2006there,
  title={Is there a link between adult neurogenesis and learning?},
  author={Leuner, Benedetta and Gould, Elizabeth and Shors, Tracey J},
  journal={Hippocampus},
  volume={16},
  number={3},
  pages={216--224},
  year={2006},
  publisher={Wiley Online Library}
}

@article{aimone2009computational,
  title={Computational influence of adult neurogenesis on memory encoding},
  author={Aimone, James B and Wiles, Janet and Gage, Fred H},
  journal={Neuron},
  volume={61},
  number={2},
  pages={187--202},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{VNHKH,
  author    = {Abhinav Vishnu and
               Jeyanthi Narasimhan and
               Lawrence Holder and
               Darren J. Kerbyson and
               Adolfy Hoisie},
  title     = {Fast and Accurate Support Vector Machines on Large Scale Systems},
  booktitle = {2015 {IEEE} International Conference on Cluster Computing, {CLUSTER}
               2015, Chicago, IL, USA, September 8-11, 2015},
  pages     = {110--119},
  year      = {2015},
  IGNOREcrossref  = {DBLP:conf/cluster/2015},
  url       = {http://dx.doi.org/10.1109/CLUSTER.2015.26},
  doi       = {10.1109/CLUSTER.2015.26},
  timestamp = {Thu, 05 Nov 2015 13:26:07 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cluster/VishnuNHKH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Bengio:2009:LDA:1658423.1658424,
 author = {Bengio, Yoshua},
 title = {Learning Deep Architectures for AI},
 journal = {Found. Trends Mach. Learn.},
 issue_date = {January 2009},
 volume = {2},
 number = {1},
 month = jan,
 year = {2009},
 issn = {1935-8237},
 pages = {1--127},
 numpages = {127},
 url = {http://dx.doi.org/10.1561/2200000006},
 doi = {10.1561/2200000006},
 acmid = {1658424},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@article{Hinton:2002:TPE:639729.639730,
 author = {Hinton, Geoffrey E.},
 title = {Training Products of Experts by Minimizing Contrastive Divergence},
 journal = {Neural Comput.},
 issue_date = {August 2002},
 volume = {14},
 number = {8},
 month = aug,
 year = {2002},
 issn = {0899-7667},
 pages = {1771--1800},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976602760128018},
 doi = {10.1162/089976602760128018},
 acmid = {639730},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{Lecun98gradient-basedlearning,
    author = {Yann Lecun and LÃ©on Bottou and Yoshua Bengio and Patrick Haffner},
    title = {Gradient-based learning applied to document recognition},
    booktitle = {Proceedings of the IEEE},
    year = {1998},
    pages = {2278--2324}
}

@article{Suykens:1999:LSS:326394.326408,
 author = {Suykens, J. A. K. and Vandewalle, J.},
 title = {Least Squares Support Vector Machine Classifiers},
 journal = {Neural Process. Lett.},
 issue_date = {June 1999},
 volume = {9},
 number = {3},
 month = jun,
 year = {1999},
 issn = {1370-4621},
 pages = {293--300},
 numpages = {8},
 url = {http://dx.doi.org/10.1023/A:1018628609742},
 doi = {10.1023/A:1018628609742},
 acmid = {326408},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {classification, linear least squares, radial basis function kernel, support vector machines},
} 

@inproceedings{Vedaldi:2015:MCN:2733373.2807412,
 author = {Vedaldi, Andrea and Lenc, Karel},
 title = {MatConvNet: Convolutional Neural Networks for MATLAB},
 booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
 series = {MM '15},
 year = {2015},
 isbn = {978-1-4503-3459-4},
 location = {Brisbane, Australia},
 pages = {689--692},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2733373.2807412},
 doi = {10.1145/2733373.2807412},
 acmid = {2807412},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, convolutional neural networks, deep learning, image understanding, machine learning},
} 

  
@InProceedings{SalHinton07,
  author=         "Ruslan Salakhutdinov and Geoffrey Hinton",
  title=          "Deep {B}oltzmann Machines",
  booktitle=      "Proceedings of the International Conference on Artificial Intelligence and Statistics",
  volume=         "5", 
  pages=		"448-455",
  year=           "2009"
}

@MASTERSTHESIS\{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
    school       = "Technical University of Denmark, {DTU} Informatics, {E-}mail: reception@imm.dtu.dk",
    address      = "Asmussens Alle, Building 305, {DK-}2800 Kgs. Lyngby, Denmark",
    type         = "",
    note         = "Supervised by Associate Professor Ole Winther, owi@imm.dtu.dk, {DTU} Informatics, and Morten M{\o}rup, mm@imm.dtu.dk, {DTU} Informatics",
    url          = "http://www.imm.dtu.dk/English.aspx",
    abstract     = "Recent findings [HOT06] have made possible the learning of deep layered hierarchical representations of data mimicking the brains working. It is hoped that this paradigm will unlock some of the power of the brain and lead to advances towards true {AI}.
In this thesis I implement and evaluate state-of-the-art deep learning models and using these as building blocks I investigate the hypothesis that predicting the time-to-time sensory input is a good learning objective. I introduce the Predictive Encoder (PE) and show that a simple non-regularized learning rule, minimizing prediction error on natural video patches leads to receptive fields similar to those found in Macaque monkey visual area V1. I scale this model to video of natural scenes by introducing the Convolutional Predictive Encoder (CPE) and show similar results. Both models can be used in deep architectures as a deep learning module."
}
 
@techreport{manyika11frontier,
  added-at = {2013-03-15T09:48:25.000+0100},
  author = {Manyika, James and Chui, Michael and Brown, Brad and Bughin, Jacques and Dobbs, Richard and Roxburgh, Charles and Byers, Angela Hung},
  biburl = {http://www.bibsonomy.org/bibtex/2cb919811c0adab2e1d359ce1bdbac9d4/sb3000},
  institution = {McKinsey Global Institute},
  interhash = {117de3444c856242c38ab48d24604b52},
  intrahash = {cb919811c0adab2e1d359ce1bdbac9d4},
  keywords = {analytics bigdata mckinsey},
  month = {June},
  timestamp = {2013-07-01T22:02:25.000+0200},
  title = {Big Data: The Next Frontier for Innovation, Competition, and Productivity},
  year = 2011
}

@article{gardner1985exponential,
  title={Exponential smoothing: The state of the art},
  author={Gardner, Everette S},
  journal={Journal of forecasting},
  volume={4},
  number={1},
  pages={1--28},
  year={1985},
  publisher={Wiley Online Library}
}

@article{deng2012three,
  title={Three classes of deep learning architectures and their applications: a tutorial survey},
  author={Deng, Li},
  journal={APSIPA transactions on signal and information processing},
  year={2012}
}

@article{DBLP:journals/corr/abs-1009-4983,
  author    = {S. M. Kamruzzaman and
               Ahmed Ryadh Hasan},
  title     = {Pattern Classification using Simplified Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1009.4983},
  year      = {2010},
  url       = {http://arxiv.org/abs/1009.4983},
  timestamp = {Mon, 05 Dec 2011 18:04:05 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1009-4983},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@ARTICLE{2016arXiv160302339V,
   author = {{Vishnu}, A. and {Siegel}, C. and {Daily}, J.},
    title = "{Distributed TensorFlow with MPI}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1603.02339},
 primaryClass = "cs.DC",
 keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
     year = 2016,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160302339V},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Wang:1993:GCC:155870.155881,
 author = {Wang, Xingwei and Zhao, Hong and Zhu, Jiakeng},
 title = {GRPC: A Communication Cooperation Mechanism in Distributed Systems},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {July 1993},
 volume = {27},
 number = {3},
 month = jul,
 year = {1993},
 issn = {0163-5980},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/155870.155881},
 doi = {10.1145/155870.155881},
 acmid = {155881},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@TECHREPORT{citeulike:13837097,
    author = {Steve Lawrence and C. Lee Giles and Ah Chung Tsoi},
    title = {What size neural network gives optimal generalization? convergence properties of backpropagation},
    institution = {NEC Research Institute and University of Queensland},
    year = {1996}
}

@Article{ref1,
author="Cybenko, G.",
title="Approximation by superpositions of a sigmoidal function",
journal="Mathematics of Control, Signals and Systems",
volume="2",
number="4",
pages="303--314",
abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
issn="1435-568X",
doi="10.1007/BF02551274",
url="http://dx.doi.org/10.1007/BF02551274",
year="1989"
}

@article{hu2016network,
  title={Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures},
  author={Hu, Hengyuan and Peng, Rui and Tai, Yu-Wing and Tang, Chi-Keung},
  journal={arXiv preprint arXiv:1607.03250},
  year={2016}
}

@article{anwar2015structured,
  title={Structured Pruning of Deep Convolutional Neural Networks},
  author={Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  journal={arXiv preprint arXiv:1512.08571},
  year={2015}
}

@article{murray2015auto,
  title={Auto-Sizing Neural Networks: With Applications to n-gram Language Models},
  author={Murray, Kenton and Chiang, David},
  journal={arXiv preprint arXiv:1508.05051},
  year={2015}
}

@article{iandola2015firecaffe,
  title={FireCaffe: near-linear acceleration of deep neural network training on compute clusters},
  author={Iandola, Forrest N and Ashraf, Khalid and Moskewicz, Mattthew W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1511.00175},
  year={2015}
}


@article{firecaffe,
		author    = {Forrest N. Iandola and
				Khalid Ashraf and
						Matthew W. Moskewicz and
						Kurt Keutzer},
		title     = {FireCaffe: near-linear acceleration of deep neural network training
				on compute clusters},
		journal   = {CoRR},
		volume    = {abs/1511.00175},
		year      = {2015},
		url       = {http://arxiv.org/abs/1511.00175},
		timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
		biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/IandolaAMK15},
		bibsource = {dblp computer science bibliography, http://dblp.org}
}
